global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Template files
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree with all parameters
route:
  # The root route group all alerts together
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  # Child route with separate receiver and additional grouping
  routes:
    # Critical alerts - go to PagerDuty and Slack immediately
    - match:
        severity: critical
      receiver: 'pagerduty'
      group_wait: 5s
      group_interval: 5s
      repeat_interval: 4h
      continue: true

    # Warning alerts - go to Slack only
    - match:
        severity: warning
      receiver: 'slack'
      group_wait: 30s
      group_interval: 30s
      repeat_interval: 24h

    # Database alerts
    - match:
        team: database
      receiver: 'database-team'
      group_wait: 15s
      group_interval: 15s
      repeat_interval: 8h
      continue: true

    # Infrastructure alerts
    - match:
        team: infrastructure
      receiver: 'infrastructure-team'
      group_wait: 15s
      group_interval: 15s
      repeat_interval: 12h
      continue: true

# Inhibition rules to prevent unnecessary notifications
inhibit_rules:
  # Don't send warning if critical alert already sent for same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']

  # Don't notify about failures if entire cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '.+'
    equal: ['cluster']

  # Don't notify about high error rate if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['service']

# Receiver configurations
receivers:
  # Default receiver - Slack
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'Observability Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        actions:
          - type: button
            text: 'View in Prometheus'
            url: 'http://prometheus:9090/graph'
          - type: button
            text: 'View in Grafana'
            url: 'http://grafana:3000/d'

  # PagerDuty receiver for critical alerts
  - name: 'pagerduty'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'CRITICAL ALERT - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: 'danger'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_KEY}'
        description: '{{ .GroupLabels.alertname }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
        links:
          - href: 'http://prometheus:9090/graph'
            text: 'Prometheus'

  # Slack receiver for warnings
  - name: 'slack'
    slack_configs:
      - channel: '#warnings'
        title: 'Warning Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: 'warning'

  # Database team receiver
  - name: 'database-team'
    slack_configs:
      - channel: '#database-alerts'
        title: 'Database Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        actions:
          - type: button
            text: 'View Grafana Dashboard'
            url: 'http://grafana:3000/d/database-monitoring'

  # Infrastructure team receiver
  - name: 'infrastructure-team'
    slack_configs:
      - channel: '#infrastructure-alerts'
        title: 'Infrastructure Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        actions:
          - type: button
            text: 'View Node Exporter Metrics'
            url: 'http://prometheus:9090/graph?g0.expr=node_cpu_seconds_total'
